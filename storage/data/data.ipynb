{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of data.csv\n",
    "\n",
    "data.csv is exactly identical to the storage_data.csv file from the [locuslab/e2e-model-learning](https://github.com/locuslab/e2e-model-learning/blob/2bf43848ac26e25321fea7bfeddf2a24135aa192/battery_storage/storage_data.csv) GitHub repo, which contains the code and data from the \"Task-based End-to-end Model Learning in Stochastic Optimization\" (Donti et al., 2017) paper.\n",
    "\n",
    "data.csv has 4 columns:\n",
    "\n",
    "`datetime`\n",
    "- type: datetime\n",
    "- hourly UTC datetime values from 2011-01-03 00:00:00 (UTC) to 2016-12-31 23:00:00 (UTC)\n",
    "- no missing or NaN values\n",
    "- because everything is UTC, we don't have to worry about daylight savings\n",
    "\n",
    "`da_price`\n",
    "- type: float\n",
    "- hourly day-ahead energy prices from PJM (\"System Energy Price Day Ahead\"), given to 2 decimals\n",
    "- units are most likely $/MWh\n",
    "- no missing or NaN values\n",
    "- see https://dataminer2.pjm.com/feed/rt_da_monthly_lmps (accessed on 2024-05-11)\n",
    "\n",
    "`load_forecast`\n",
    "- type: float\n",
    "- hourly load forecast for the \"RTO\" Forecast Area, in MW\n",
    "- forecasts are evaluated at 15:45 UTC time on the previous day. For example, if `datetime` is `2016-05-05 18:00:00`, then `load_forecast` is the forecast generated at `2016-05-04 15:45` (UTC) for `2016-05-05 18:00:00`.\n",
    "- no missing or NaN values\n",
    "- see https://dataminer2.pjm.com/feed/load_frcstd_hist (accessed on 2024-05-11)\n",
    "\n",
    "`temp_dca`\n",
    "- type: float\n",
    "- temperature in Farenheit\n",
    "- missing 40 values\n",
    "- unclear where/how these values were obtained\n",
    "\n",
    "\n",
    "We largely follow the data processing pipeline from the locuslab/e2e-model-learning repo, with the following changes:\n",
    "* We keep all price values, including prices > $500/MWh, which Donti et al. marked as outliers.\n",
    "* We do not include `past temp^2`, `future temp^2`, or `future temp^3` as features.\n",
    "\n",
    "This notebook processes data.csv and saves labels (energy prices) and standardized features in data.npz. This .npz file has four keys:\n",
    "- `'X'`: shape [2189, 101], type float64, standardized features\n",
    "- `'Y'`: shape [2189, 24], type float64, energy prices\n",
    "- `'X_mean'`: shape [101], type float64, mean of each feature before standardization\n",
    "- `'X_std'`: shape [101], type float64, std-dev of each feature before standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "import pytz\n",
    "\n",
    "# hide top and right splines on plots\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz = pytz.timezone('America/New_York')\n",
    "df = pd.read_csv('data.csv', parse_dates=[0])\n",
    "\n",
    "display(df)\n",
    "print(df.dtypes)\n",
    "\n",
    "# df has 4 columns:\n",
    "#\n",
    "# datetime         datetime64[ns]\n",
    "# da_price                float64\n",
    "# load_forecast           float64\n",
    "# temp_dca                float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_da_price'] = np.log(df['da_price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate and plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(df: pd.DataFrame) -> None:\n",
    "    # verify no missing datetimes\n",
    "    expected_dts = pd.date_range(datetime(2011, 1, 3, 0, 0, 0), datetime(2016, 12, 31, 23, 0, 0), freq=timedelta(hours=1))\n",
    "    actual_dts = set(df['datetime'])\n",
    "    assert len(expected_dts.difference(actual_dts)) == 0\n",
    "\n",
    "    # verify no NaNs\n",
    "    for col in ('datetime', 'da_price', 'load_forecast'):\n",
    "        assert df[col].isna().sum() == 0\n",
    "\n",
    "    # print number of NaNs in 'temp_dca'\n",
    "    print('# of NaNs in `temp_dca`:', df['temp_dca'].isna().sum())\n",
    "\n",
    "\n",
    "validate_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(df: pd.DataFrame) -> None:\n",
    "    cols_and_units = (\n",
    "        ('da_price', '$/MWh'),\n",
    "        ('log_da_price', 'log($/MWh)'),\n",
    "        ('load_forecast', 'MW'),\n",
    "        ('temp_dca', 'Â°F')\n",
    "    )\n",
    "\n",
    "    _, axs = plt.subplots(4, 1, figsize=(12, 8), sharex=True, tight_layout=True)\n",
    "    for i, ax in enumerate(axs):\n",
    "        col, units = cols_and_units[i]\n",
    "        ax.plot(df['datetime'], df[col], lw=0.5, color=plt.cm.tab10.colors[i])\n",
    "        ax.set(ylabel=units)\n",
    "\n",
    "    _, axs = plt.subplots(1, 4, figsize=(12, 3), tight_layout=True)\n",
    "    for i, ax in enumerate(axs):\n",
    "        col, units = cols_and_units[i]\n",
    "        ax.hist(df[col], bins=100, color=plt.cm.tab10.colors[i])\n",
    "        ax.set(xlabel=units, ylabel='count', title=col)\n",
    "\n",
    "\n",
    "plot_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['datetime'].dt.date\n",
    "df['hour'] = df['datetime'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices = df.pivot(index='date', columns='hour', values='da_price')\n",
    "df_logprices = df.pivot(index='date', columns='hour', values='log_da_price')\n",
    "df_load = df.pivot(index='date', columns='hour', values='load_forecast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.pivot(index='date', columns='hour', values='temp_dca')\n",
    "df_temp = df_temp.transpose().bfill().ffill().transpose()\n",
    "\n",
    "assert df_logprices.index.equals(df_load.index)\n",
    "assert df_logprices.index.equals(df_temp.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dates = df_logprices.index.to_series()\n",
    "\n",
    "holidays = USFederalHolidayCalendar().holidays(\n",
    "    start='2011-01-01', end='2017-01-01').date\n",
    "\n",
    "df_feat = pd.DataFrame({\n",
    "    'weekend': df_dates.map(lambda x: x.isoweekday() >= 6),\n",
    "    'holiday': df_dates.isin(holidays),\n",
    "    'dst': df_dates.map(\n",
    "        lambda x: tz.localize(datetime.combine(x, datetime.min.time())).dst().seconds > 0\n",
    "    ),\n",
    "    \"cos_doy\": df_dates.map(lambda x: np.cos(x.timetuple().tm_yday/365*2*np.pi)),\n",
    "    \"sin_doy\": df_dates.map(lambda x: np.sin(x.timetuple().tm_yday/365*2*np.pi))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a single training example (x,y) has\n",
    "# x = [\n",
    "#     previous day's 24 hourly log-prices,\n",
    "#     today's 24 hourly load forecast,\n",
    "#     previous day's 24 hourly temperature,\n",
    "#     today's 24 hourly temperature,\n",
    "#     other features about today (is_weekend, is_holiday, is_dst, cos_doy, sin_doy)\n",
    "# ]\n",
    "# len(x) = 4 * 24 + 5 = 101\n",
    "\n",
    "# y = [today's 24 hourly prices]\n",
    "# len(y) = 24\n",
    "\n",
    "X = np.hstack([df_logprices.iloc[:-1].values,  # past log-price\n",
    "               df_load.iloc[1:].values,        # future load forecast\n",
    "               df_temp.iloc[:-1].values,       # past temp\n",
    "               df_temp.iloc[1:].values,        # future temp\n",
    "               df_feat.iloc[1:].values]).astype(np.float64)\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "X = (X - X_mean) / X_std\n",
    "\n",
    "Y = df_prices.iloc[1:].values\n",
    "\n",
    "dates = np.array(df_prices.iloc[1:].index).astype('datetime64[D]')\n",
    "\n",
    "np.savez_compressed('data.npz', X=X, Y=Y, X_mean=X_mean, X_std=X_std, dates=dates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dauq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
